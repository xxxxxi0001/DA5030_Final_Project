---
title: "**Ensemble Learning for Heart Disease Prediction Using Classification Models**"
subtitle: "DA5030"
date: "Fall 2025"
author: "Zhang, Xi"
output: 
  html_document:
    toc: true
    toc_float: true
---


### Outline

**1. Introduction**

**2. Functions**

**3. Data Cleaning**

**4. Logistic Regression Model**

  - *4.1. Feature Engineering* 
  
  - *4.2. Modeling*
  
  - *4.3. Tuning*

**5. Random Forest Model**

  - *5.1. Modeling *

  - *5.2. Tuning*

**6. C5.0 Model** 

  - *6.1. Modeling* 
  
  - *6.2. Tuning*

**7. Ensembling 1 (Bag) (Logistic Regression, Random Forest, C5.0)**

  - *7.1. Modeling* 

  - *7.2. Tuning* 

  - *7.3. Final Validation*

  - *7.4. Use Ensemble 1 Make Prediction*

**8. Ensembling 2 (Stack) (Logistic Regression, Random Forest, C5.0)**

  - *8.1. Modeling* 

  - *8.2. Tuning* 

  - *8.3. Final Validation*

  - *8.4. Use Ensemble 2 Make Prediction*

**9. Model Evaluation**

**10. Conclusions**

**Reference**

```{r echo=FALSE, message=FALSE}
# Load all packages, chunck not shown as suggested by prof
library(knitr)
library(randomForest)
library(C50)
```


--------------------------------------------------------------------------------


# **1. Introduction**
This project follows the CRISP-DM process: define problem, understand and preparing the data, build and evaluate multiple models, and consider why model should be deployed in a clinical context.

The HeartDisease Data Set I choose for this project come from Kaggle and can be visit with this link: https://www.kaggle.com/datasets/tan5577/heart-failure-dataset
This data set is designed for classification. I made a copy of this data set through google sheet so it can be imported with URL link. 

HeartDisease is a worldwide problem with very little observable features until it becomes serious. Therefore if we can use clinical available data set to make a prediction and give potential patient's a head up for earlier warning, it can be very helpful. Patient with potential risk can benefit from this model. My goal is to build a predictive model to classify has or does not has heart disease based on clinical available data set.

This is a very small data set with few samples & fewer features, therefore PCA is not that helpful since PCA's purpose is to reduce dimension. (PCA is useful when facing many samples with many features & multicollinearity occur). But I still perform PCA in Bag Logistic Regression because there is a strong correlation between new derived feature DoubleRISK & Oldpeak.

This data set has NA value, I made different imputation based on their distribution & number of missing value (detailed explanation provide below).

I choose Logistic Regression, Random Forest and C5.0 because in this data set, we have both numerical feature & categorical feature. These three models can handle this type data set better than Naive Bayes. I didn't choose kNN is because I already use kNN imputation, I think it might be considered as data leakage.

In this project, my working pipeline is Data Cleaning, Logistic Regression Model Construction, Random Forest Model Construction, C5.0 Model Construction, Final Ensemble Model Construction & Model Evaluation.

Each Function in section *Functions* serve to perform different purposes:

1. `check_na_zero`: check each column if they have zero or NA values.
2. `replace_na_with_zero`: for later kNN imputation & median imputation, it is required that all missing value to be NA. This function serve to replace all zero that need to be considered as NA into NA so they can be imputated with kNN or median
3. `median_imputation`: for normally distributed feature that only have few missing values, replace their missing value with median

4. `automation_knn_imputation` (`initialize_distance_find_best_k`, `initialize_not_na_index`, `initialize_test_k_index`, `find_best_k`, `kNN_Imputation`): 
  - `initialize_distance_find_best_k`: serve to initialize a matrix with euclidean distance
  - `initialize_not_na_index`: serve to get index that does not have NA value so they can be used in testing best k
  - `initialize_test_k_index`: randomly select certain portion of index from not na index to test best k
  - `find_best_k`: use the k index to test what is the best k for kNN imputation for specific feature so it introduce least error
  - `kNN_Imputation`: use features get earlier to impute NA with kNN method
  - `automation_knn_imputation`: serve to act as a dummy function to call previous functions so users won't be confused

5. `z_score_outlier`: for normally distributed feature that need outlier treatment, get their outliers with z score and replace outlier above upper boundary with upper boundary, below lower boundary with lower boundary.
6. `IQR_outlier`: For not normally distributed feature, their outliers suggests to be treated with IQR therefore can eliminate the bias come from outliers. 
7. `check_multicollinearity`: check if strong correlation happened among these features and output the result in a clean way so user won't be confused
8. `three_set_partition`: in this function, I used method stratified splitting so class imbalance won't be instroduced. I collect positive, negative index and randomly select index based on assigned portion and combine them back together so each has same portion of negative/positive as original dataset. Then I shuffle them so it won't be one category concentrate in front the other concentrate on last.
9. `ensemble_train_partition`: for building an ensemble model, we randomly select certain portion of index from early partition train index. This serve to automatically gives you such list of index.
10. `check_class_imbalance`: check if class imbalance happen that their positive/negative are not equal
11. `backward_p_lr`: construct logistic regression model that only keep feature that contribute significantly to the model (p<0.5), input can be

12. Cross Validation (`k_stratified_cv`, `cross_validation`):
  - `k_stratified_cv`: split train index into k fold index for later cross validation
  - `cross_validation`: test different index fold's performance, serve to test if pipeline is valid. This function support logistic regression, random forest, C5.0 & rpart decision tree

13. `make_ensemble_predict`: serve to use a list of model (support logistic regression, random forest, C5.0, rpart) to make ensemble prediction with a list of data frame (have to align with model list, one model, one list, will send warning if not). If model use only one set of data frame, this model also support use one data frame make prediction.
14. `find_best_threshold`: use test index find the best threshold for this model to make prediction on this data set, loop over all possible threshold
15. `ensemble_weight_F1`: serve to calculate how each model should contribute to final prediction with a list of model & a list of data frame that align with model list. User don't need to make prediction themselves, a list of model is fine. But the length of model must be same with length of data frame, will send warning if not. If model use only one set of data frame, this model also support use one data frame make prediction. (support logistic regression, random forest, C5.0, rpart)
16. `ensemble_result_with_weight`: Use weight list generated before re-build model with list of model and list of data frame that aligned with model. Model length, weight length & data frame list must be in same length, will send warning if not. However, if only one data frame is generated, this function also support use one data frame make prediction. But if different data frame is used for different models, user need to make sure to generate a list of data frame themselves. (support logistic regression, random forest, C5.0, rpart)

17. Stack Model with glm
  - `generate_stack_df`: a helper function to generate a data frame suit for stack model generation & sending warning if model and data frame's length are not equal & tell user we only support logistic regression, random forest, C5.0 & rpart model.
  - `stack_model_lr`: use stack data frame generate stack model with glm
  - `stack_model_predict_lr`: use stack model make prediction

In *Data Cleaning*, I checked data frame, each feature's meaning and their range. Fasting Blood Sugar & Heardiease's one zero value has logical meaning not numerical meaning so I convert them into factor. I notice feature Oldpeaks has negative value so I performed a right shift to drag it into all positive value so we can perform transformation in later treatment. I performed z-score outlier treatment for normally distributed feature & IQR outlier treatment for not normally distributed feature. Two features have missing values, Resting Blood Pressure & Chlesterol. Resting Blood Pressure is normally distributed & only has 1 missing value so I performed median imputation. Chlesterol is also normally distributed but it has 172 missing value (large portion compare to total 918 samples), therefore I think median imputation will introduce bias to this feature so I perform kNN imputation. Now we have treated features ready for C5.0 & Random Forest. Random Forest use bagging so its not sensitive to outliers so I keep all raw data except NA treated feature. C5.0 is sensitive to NA & extreme outliers, so I keep NA & outlier treated features.

In *Logistic Regression Model*, I have three subset, Feature Engineering, Modeling & Tuning. In *Feature Engineering*, I first derived new feature from Max Heart Rate & Oldpeaks because if Max Heart Rate & Oldpeaks both showed an increment, it could refer to a potential risk. By checking each feature's distribution, I notice feature Age & Oldpeaks are little left skewed. For left skewed feature, transformation like square, or square(x+1) (if have zero value) is recommended, therefore I perform square transformation for these two features. I notice newly derived feature Double Risk is right skewed so I perform square root. I normalize all feature so they make same contribution therefore make model easier to interpret. I check Multicollinearity, doubleRISK_sqrt & Oldpeak_square are highly correlated. Therefore I perform PCA to reduce Multicollinearity because PCA meant to find high variance (meaning less to little correlation). I notice PC1~PC5 contribute 99% variance so I replace all my numerical feature with PC1~PC5. For categorical feature with more than 2 choices, I use one-hot encode because none of them are ordinal. For feature with 2 choices like sex & ExerciseAngina, I one/zero encode them cause they are also not ordinal. Now we have a data set ready for Logistic Regression Model. In *Modeling*, I partition three data set, 50% for training & cross validation (I think 50% is enough for stable model fitting), 25% for testing & tuning (tuning is important if we want model perform lean toward our expectation) 25% for final validation to test model performance (to ensure an unbiased performance evaluation). No class imbalance occur cause I use stratified split (stratified split ensure equal portion of positive & negative are selected therefore same as original data set which avoid class imbalance). Because this is a very small data set, I use cross validation to test pipeline & how trustworthy is our model. (If the performance of model aligned with cross validation's result, we can call the model's performance trustworthy) I made a bagged model for logistic regression out of 10 logistic regression model and use bagged model make prediction by calculate their mean. The result aligned with cross validation with threshold 0.5 therefore the performance is trustworthy. In *Tuning*, I loop over all threshold with test data set and find the threshold that gives most F1 value (because F1 find balance between precision & recall so model won't be too agressive or too lay back in predicting positive feature) & I calculate their weight by portion. I time each model prediction with their weight & sum together as final prediction probability and use best threshold make final prediction. 

In *Random Forest Model*, I have two subset, Modeling & Tuning. In *Modeling* I use cross validation to check pipeline. Then I construct a random forest with 500 trees & 3 mtry as suggested mtry number is sqrt(feature) or 1/3(features) or lg(features) (I used sqrt(features) in tree construction cause I didn't observe any difference when I change this feature & its suggested) (Ellis, 2022). Then in *Tuning*, I use function find_best_threshold to find a better threshold that will suit better for the model. The model perform similar with cross validation before & after tuning, the result is trustworthy.

In *C5.0 Model*, I have two subset, Modeling & Tuning. In *Modeling* I use cross validation to check pipeline. Then I construct a C5.0 model with 10 trials and from summary I learned the model is good because its false prediction is only 4.1%. I use probability prediction because I want to ensemble the result & I want the result could run in my function, check_model_performance & find_best_threshold. Then in *Tuning*, I use function find_best_threshold to find a better threshold that will suit better for the model. The model perform similar with cross validation before & after tuning, the result is trustworthy.

In *Ensembling 1 (Bag) (Logistic Regression, Random Forest, C5.0)*, I have three subset, Modeling, Tuning & Final Validation. In *Modeling*, I use different model make prediction with different data set and calculate their mean as ensemble result. Then I use 0.5 as threshold to check their performance. In *Tuning*, I find best threshold with ensemble result & calculate each model's weight in portion. I use prediction result time weight (in portion) as my final prediction result. In *Final Validation*, I use validation data set to make prediction, give prediction different weight (based on my calculation before) & check its performance with the best threshold I find with test index. I also made an example of how to run one patient with bag ensemble model.

In *Ensembling 2 (Stack) (Logistic Regression, Random Forest, C5.0)*, I have three subset, Modeling, Tuning & Final Validation. In *Modeling*, I use different model generate a stack model and I use 0.5 as threshold to check its performance. In *Tuning*, I find best threshold with stack's result from test index. In *Final Validation*, I use validation data set to make prediction with threshold I find with test index. I also made an example of how to run one patient with stack ensemble model.


# **2. Functions**


## Check NA & zero (func)
```{r}
# Input: 1). df: data frame
# Return: nothing, but will show if data frame has zero/NA value
check_na_zero<-function(df){
  
  # Initialize for total number of zero/na value
  total_count<-0
  
    # Ignore non-numerical column
    for (i in 1:ncol(df)) {
    if (is.numeric(df[[i]])){
    
      # Count NA, if appear, show which column has NA
      na_count <-sum(is.na(df[[i]]))
      # Add up add NA/zero value incase no zero/NA appear
      total_count<-na_count+total_count
      if (na_count>0) {
        cat("There are total", na_count ,"NA value in column", colnames(df)[i],"\n")
        }
    
      # Count zero value, if appear, show which column has zero value
      zero_count<-length(which(df[[i]] == 0))
      # Add up add NA/zero value incase no zero/NA appear
      total_count<-zero_count+total_count
      if (zero_count>0) {
        cat("There are total", zero_count, "Zero value in column", colnames(df)[i], "\n")
        }
      }
    }
  
  # if no na/zero value detect, output this information
  if (total_count==0) {
    cat("This data set does not have any zero value or NA value for its numerical features!\n")
  }
}
```


## Zero into NA (func)
```{r}
# Input: 1). df: data frame with zero value 
#        2). ignore_cols: zero value that does not need to be changed into NA
# Return:1). df: data frame with designated columns' zero value replaced into na
replace_na_with_zero<-function(df,ignore_cols) {
  
  # For only column that their zero value need to change into NA
  for (i in 1:ncol(df)) {
  if (is.numeric(df[[i]]) && !(colnames(df)[i] %in% ignore_cols)){
     
    # If their zero value is above 0, return result & change them into NA
    zero_count<-length(which(df[[i]] == 0))
    if (zero_count>0) {
      cat("Total", zero_count, "Zero value in column", colnames(df)[i],"has successfully changed into NA","\n")
      df[[i]][df[[i]]==0]<-NA
      }
    }
  }
 return(df) 
}
```


## Median Imputation (func)
```{r}
# Input: 1). df: the data frame that has NA need median imputation treatment
#        2). target_col: columns name that need its NA be treated with median imputation
# Return:1). df: data frame with designated column successfully impute
# Usage: 1). list of column: df<-median_imputation(df,c("a","b"))
#        2). one column: df<-median_imputation(df,"a")
median_imputation<-function(df,target_col) {
  
  # loop over each column and perform median imputation
  for (i in target_col) {
    m_value<-median(df[[i]], na.rm=TRUE)
    df[[i]][is.na(df[[i]])]<-m_value
      
    cat("Feature",i,"is successfully imputated with median \n")
  }
  
  # return data frame
  return(df)
}
```


## kNN Imputation (func 1~6)
### kNN Imputation_initialize euclidean distance matrix (func 1)
```{r}
# Input: 1). df: data frame that need to be treated
# Return:1). df_distance: the euclidean distance list that will be used
initialize_distance_find_best_k<-function(df){
  
  # Find all numerical column
  numerical_columns<-names(df)[sapply(df,is.numeric)]
  # Find all numerical col with NA
  excluded_columns<-names(df)[colSums(is.na(df))>0]
  # Exlude column with NA
  target_columns<-setdiff(numerical_columns,excluded_columns)
  # Normalization & calculate euclidean distance
  df_matrix<-as.matrix(df[,target_columns])
  df_matrix<-scale(df[, target_columns])
  df_distance<-as.matrix(dist(df_matrix, method="euclidean"))
  return(df_distance)
}
```

### kNN Imputation_initialize target features' index that are not NA value (func 2)
```{r}
# Input: 1). target_feature: specific feature that need to find not na value
# Return:1). not_na_index: target_feature's all index that are not na
initialize_not_na_index<-function(target_feature){
    
    # Find index that are not na
    not_na_index<-which(!is.na(target_feature))
    return(not_na_index)
}
```

### kNN Imputation_initialize target features' index that are not NA value (func 3)
```{r}
# Input: 1). test_proportion: proportion you want to select from not_na_index to test best k
#        2). not_na_index: the not_na_index you get from previous function
# Return:1).test_k_index: the index you wanna use to test best k
initialize_test_k_index<-function(test_proportion,not_na_index) {
  
  # From index does not have na, randomly select certain portion of index 
  test_k_index<-sample(not_na_index,size=floor(test_proportion*length(not_na_index)))
  return(test_k_index)
}
```

### kNN Imputation_find best k for target feature (func 4)
```{r}
# Input: 1). max_k: number of k you wanna test
#        2). test_k_index: the index you wanna use to test best k
#        3). df_distance: the euclidean distance matrix build before
#        4). not_na_index: the not_na_index you get from previous function
#        5). target_feature: the feature need to find best k
# Return:1). smallest_k used for kNN imputation
find_best_k<-function(max_k,test_k_index,df_distance,not_na_index,target_feature){
  
  # Initialize
  smallest_RMSE<-Inf
  smallest_k<-Inf
  
  
  # We gonna test k from 1 to 20
  for (k in 1:max_k) {
  
    # Initialization for total standard error
    se_total<-0

    # loop over all test k index that randomly selected earlier
    for (j in test_k_index){
      
      # Initialize
      df_temp<-df_distance
      # Set self to Inf so it won't be considered in smallest euclidean distance 
      df_temp[j,j]<-Inf
    
      # Get all distance of this specific index
      total_distance<-df_temp[j,not_na_index]
    
      # Ordered them the distance
      ordered_dist<-order(total_distance)
    
      # Choose the closest k's distance and get their original index in original data set
      original_index<-not_na_index[ordered_dist][1:k]
    
      # and calculate closest k's distance's mean value
      # This is our prediction value
      test_value<-round(mean(target_feature[original_index]))
    
      # Get actual value
      actual_value<-target_feature[j]
    
      # Add up the standard error for later RMSE calculation 
      se_total<-se_total+(actual_value-test_value)^2
    }
  
    # Get RMSE value for prediction's accuracy
    RMSE<-sqrt(se_total/length(test_k_index))
  
    # If this k's RMSE is smaller than any previous RMSE
    if (RMSE < smallest_RMSE){
    
      # update RMSE
      smallest_RMSE<-RMSE
    
      # update k
      smallest_k<-k
    }
  }
  # print out the best k we find and have its RMSE as additional value
  cat("The best k is", smallest_k,"which give smallest error of", smallest_RMSE,"\n")
  
  # remove temporary vector
  rm(df_temp,total_distance)
  
  # clear vector storage
  gc()
  
  return(smallest_k)
}
```

### kNN Imputation_use the best k perform kNN imputation (func 5)
```{r}
# Input: 1). df: data frame that need NA kNN imputation treatment
#        2). smallest_k: the best k calculate before
#        3). target_feature: specific column that need NA kNN imputation
#        4). df_distance: the euclidean distance matrix made before
# Return:1). target_feature: feature value that successfully imputated
kNN_Imputation<-function(df,smallest_k,target_feature,df_distance){
  # Initialization
  k<-smallest_k
  na_cols<-which(is.na(target_feature))
  na_rows<-which(is.na(target_feature))
  df_temp<-df_distance
  
  # For all values in target_feature
  for (i in na_rows) {
  
    # Ignore self
    df_temp[i,i]<-Inf
  
    # Ignore self's NA value
    df_temp[i,na_cols]<-Inf
  
    # Get smallest best k euclidean distance
    closest_k<-order(df_temp[i,])[1:k]
  
    # Impute the mean value of these k numbers
    target_feature[i]<-round(mean(target_feature[closest_k]))
  }
  return(target_feature)
}
```

### Automation kNN Imputation to Free Hand (func 6)
```{r}
# Input: 1). df: the data frame that has NA need kNN imputation treatment
#        2). test_por=0.1: 10% of total data set will be used for k testing
#        3). max_k=20: will test 20 k for best k
#        2). ignore_cols: columns name that does not need its NA be treated
# Return:1). df: data frame with designated column successfully impute
# Usage Example: df<-automation_knn_imputation(df,ignore_col=c("a","b"))
automation_knn_imputation<-function(df,test_por=0.1,max_k=20,ignore_col=NULL) {
  
  # Collect all column name with NA
  na_col_name<-names(df)[colSums(is.na(df))>0]
  
  # If there are columns want to be ignored, remove them from column name
  if (!is.null(ignore_col)) {
    na_col_name<-setdiff(na_col_name,ignore_col)
  }
  
  # For all column's that their NA need to be impute, call functions sequentially 
  # This function will tell you which column it is working on & how it plans to work on
  for (i in na_col_name) {
    target_feature<-df[[i]]
    df_distance<-initialize_distance_find_best_k(df)
    not_na_index<-initialize_not_na_index(target_feature)
    test_k_index<-initialize_test_k_index(test_por,not_na_index)
    cat("Now is processing column",i,"\n")
    smallest_k<-find_best_k(max_k,test_k_index,df_distance,not_na_index,target_feature)
    df[[i]]<-kNN_Imputation(df,smallest_k,target_feature,df_distance)
    cat("Feature",i,"is successfully imputed with kNN. \n")
    
    # Remove unncessary information to release storage
    rm(df_distance)
    gc()
  }
  return(df)
}
```


## z-score Outlier Treatment (func)
```{r}
# Input: 1). df: the data frame that has feature need outlier treatment 
#        2). variable: variables that need outlier treatment & normally distributed
# Return:1). df: data frame with designated variables successfully treated 
# Usage: 1). list of column: df<-z_score_outlier(df,c("a","b"))
#        2). one column: df<-z_score_outlier(df,"a")
z_score_outlier<-function(df,variable) {
  
  # for each variable 
  for (i in variable) {
    
    # calculate their mean, standard deviation and z-score
    mean_v<-mean(df[[i]],na.rm = TRUE)
    sd_v<-sd(df[[i]],na.rm = TRUE)
    z_v<-(df[[i]]-mean_v)/sd_v
    
    # calculate upper boundary & lower boundary
    up_b<-mean_v+3*sd_v
    lower_b<-mean_v-3*sd_v
    
    # Get how many outliers detected for output result
    outlier<-sum(z_v > 3,na.rm = TRUE)+sum(z_v < -3,na.rm = TRUE)
    
    # Replace upper outliers with upper boundary, lower outliers with lower boundary
    df[[i]][z_v > 3]<-up_b
    df[[i]][z_v < -3]<-lower_b
    
    # Output result
    cat("Z-Score: There are total",outlier, "outliers detected for feature", i, ". Above upper boundary ", round(up_b,2), "is replaced into ", round(up_b,2), ". Below lower boundary", round(lower_b,2), "is replaced into", round(lower_b,2), "\n")
  }
  
  # return data frame
  return(df)
}
```


## IQR for Outliers (func)
```{r}
# Input: 1). df: Data Frame with Outliers
#        2). variables: Features in Data Frame that need outlier Treatment
# Return:1). df: return data frame with variables' outliers treated
IQR_outlier<-function(df,variables){
  for (i in variables) {
  
  # Identify first quartile, third quartile and IQR
  Q1<-quantile(df[[i]],0.25)
  Q3<-quantile(df[[i]],0.75)
  IQR<-Q3-Q1
  
  # Identify lower quartile and upper quartile
  low_q<-Q1-1.5*IQR
  up_q<-Q3+1.5*IQR
  
  # Collect outliers information
  outlier<-sum(df[[i]]<low_q) + sum(df[[i]]>up_q)

  # Get 5% value and 95% value for replacement
  caps<-quantile(df[[i]],probs=c(0.05,0.95))
  
  # For below lower quartile replaced by 5%
  df[[i]][df[[i]]<low_q]<-caps[1]
  # For above upper quatile repplaced by 95%
  df[[i]][df[[i]]>up_q]<-caps[2]
  
  cat("IQR: There are total",outlier, "outliers detected for feature", i, ". Above upper quartile ", up_q, "is replaced into ", caps[2], ". Below lower quartile", low_q, "is replaced into", caps[1], "\n")
  }
  return(df)
}
```


## Check Multicollinearity (func)
```{r}
# Input: 1). variables_df: variables selected for multicollinearity checking (has to be numerical)
#        2). threshold: above what correlation value is considered as strong correlation
# Return:Nothing will return but will tell you what features are highly correlated
check_multicollinearity<-function(variables_df, threshold=0.8) {
  
  # create a data frame of correlational relationship between these variables
  cor_df<-as.data.frame(as.table(cor(variables_df,use="complete.obs")))

  # Remove Duplicated Rows
  remove_rows<-c()
  
  # Loop over all row 1 and row 2 (variable name), 
  # row 1 variable is equal to row 2 variable & row 2 variable is also equal 
  # to row 1 variable & this happen the first time, collect in remove rows
  for (i in 1:nrow(cor_df)) {
    for (j in 1:nrow(cor_df)) {
      if (as.character(cor_df[i,1])==as.character(cor_df[j,2]) && as.character(cor_df[i,2]) == as.character(cor_df[j,1]) && i<j) {
        remove_rows<-c(remove_rows,j)
      }
    }
  }
  
  # and remove them
  cor_df<-cor_df[-remove_rows, ]
  
  total_count<-0
  
  # Display Result
  for (i in 1:nrow(cor_df)) {
    
    # If greater than threshold and is not happened between self & self
    # Threshold set as 0.8, you can change this feature
    # Return the result
    if (abs(cor_df[i,3])>=threshold & cor_df[i,1] != cor_df[i,2]) {
      cat("There is a strong correlational relationship (", round(as.numeric(cor_df[i,3]),2),") occur betewen",as.character(cor_df[i,1]),"and",as.character(cor_df[i,2]),".\n")
      total_count<-total_count+1
    }
  }
  if (total_count==0) {
    cat("There is no multicolineariality detect in this data set!")
  }
}
```


## Partition in train, test, validation data set with stratified spliting (func)
```{r}
# Input: 1). df: data frame that used for partition
#        2). target_col: specific column that you don't want to fall in class
#            imbalance (usually column used for prediction)
#        3). train_portion: portion of data select for training
#        4). test_portion: portion of data select for testing 
#            (rest for validation)
#        5). positive: target column's positive value
#        6). negative: target column's negative value
# Return:1). A list contain Training Index, Test Index & Validation Index
three_set_partition<-function(df, target_col, train_portion, test_portion,positive,negative){

  # Get positive/negative index
  p_index<-which(df[[target_col]]==positive)
  n_index<-which(df[[target_col]]==negative)

  # initialization for partition
  n_p<-length(p_index)
  n_n<-length(n_index)

  # Randomly selected designated train index
  p_train<-sample(p_index,size=floor(train_portion*n_p))
  n_train<-sample(n_index,size=floor(train_portion*n_n))
  train_index<-c(p_train,n_train)
  # shuffle
  train_index<-sample(train_index)

  # Get rest index
  rest_p_i<-setdiff(p_index,p_train)
  rest_n_i<-setdiff(n_index,n_train)
  
  # For rest
  nr_p<-length(rest_p_i)
  nr_n<-length(rest_n_i)

  # choose designated portion for testing
  test_num<-test_portion/(1-train_portion)
  p_test<-sample(rest_p_i,size=floor(test_num*nr_p))
  n_test<-sample(rest_n_i,size=floor(test_num*nr_n))
  test_index<-c(p_test,n_test)
  # shuffle
  test_index<-sample(test_index)

  # rest is validation
  p_val<-setdiff(rest_p_i,p_test)
  n_val<-setdiff(rest_n_i,n_test)
  validation_index<-c(p_val,n_val)
  # shuffle
  validation_index<-sample(validation_index)
  
  # Return train, test, validation index as a list
  return(list(
    train_index=train_index,
    test_index=test_index,
    validation_index=validation_index
  ))
}
```


## Create Ensemble Data Set (func)
```{r}
# Input: 1). df: data frame that used for training partition
#        2). train_index: the overall training index that you gonna selected from
#        3). sub_num: number of ensemble list
#        4). sub_portion: portion of overall training you want to select as sub-list
#        5). target_col: the column that need to be predicted 
#        6). positive: target_col's positive value
#        7). negative: target_col's negative value
# Return:1). A list contain sub_num number of training index
ensemble_train_partition<-function(df,train_index,sub_num,sub_portion,target_col,positive,negative) {
  
  # Initialize a list to store partition index
  partitions<-vector("list",sub_num)
  
  # Get train_index
  train_label<-df[[target_col]][train_index]
  
  # Get positive negative's index
  positive_index<-train_index[train_label==positive]
  negative_index<-train_index[train_label==negative]
  
  # Randomly choose designated number (sub_num) of subset from positive/negative
  # index with designated portion (sub_portion) of number
  # therefore it won't fall in class imbalance
  for (i in 1:sub_num){
    nt_p<-length(positive_index)
    nt_n<-length(negative_index)
    subset_train_index_p<-sample(positive_index,size=floor(sub_portion*nt_p),replace=TRUE)
    subset_train_index_n<-sample(negative_index,size=floor(sub_portion*nt_n),replace=TRUE)
    # clollect selected index in partition list with shuffle so positive/negative 
    # won't cluster together
    partitions[[i]]<-sample(c(subset_train_index_p,subset_train_index_n))
  }
  
  # Return a list of partition index
  return(partitions)
}
```


## Check Class Imbalance (func)
```{r}
# Input: 1). df: the data frame those index fall into
#        2). index_list: the list of index you want to check if fall into class imbalance
#        3). target_col: the column that used to check class imbalance 
#        4). positive: positive value of target_col
#        5). negative: negative value of target_col
# Return: No Return, but will output result
check_class_imbalance<-function(df,index_list,target_col,positive,negative) {
  
  # If index list is a list of index list
  if (is.list(index_list)) {
    for (i in 1:length(index_list)) {
      
      # Loop over each list & change each list into serial of 
      # numerical value & calculate portion of positive/negative value, return result
      index<-unlist(index_list[[i]])
      t<-round(mean(df[[target_col]][index]==positive)*100,2)
      f<-round(mean(df[[target_col]][index]==negative)*100,2)
      cat("In partition",i, "of", deparse(substitute(index_list)),"There are", t, "% postive people in training data set and",f,"% negative people in training data set.","\n")
    }
  }
  
  # If index list is only one list of number
  if (is.numeric(index_list)) {
    
    # calculate portion of positive/negative value, return result
    index<-unlist(index_list)
    t<-round(mean(df[[target_col]][index]==positive)*100,2)
    f<-round(mean(df[[target_col]][index]==negative)*100,2)
    cat("In",deparse(substitute(index_list)),"There are", t, "% positive people in training data set and",f,"% negative people in training data set.","\n")
  }
}
```


## Backward p-value for Logistic Regression Model (func)
```{r}
# Input: 1). df: the data frame those index fall into
#        2). train_index: the index for ensemble/not ensemble train 
#        3). target_col: the target column that need to be predicted
#        4). positive: positive value of target_col
#        5). positive_weight: if class imbalance within data set, add weight based on portion
#        6). negative_weight: if class imbalance within data set, add weight based on portion 
# Return:1). A list of trained emseble Logistic Model Based on List of Index you provide
backward_p_lr<-function (df,train_index,target_col,positive,positive_weight,negative_weight) {
  
  # For index list create for ensemble model, create ensemble model 
  if (is.list(train_index)) {
    logistic_model<-list()
    for (i in 1:length(train_index)) {
      
      # Initialize Index of Training
      index<-unlist(train_index[[i]])
      
      # Initialize Model
      logistic_model[[i]]<-glm(as.formula(paste(target_col, "~ .")), data=df[index,], family=binomial, weights = ifelse(df[[target_col]][index] == positive, positive_weight, negative_weight))
      
      # Initialize Coefficient
      coeffecient<-summary(logistic_model[[i]])$coefficients
      
      # Initialize p-value list
      p_values<-coeffecient[-1,"Pr(>|z|)"]
    
      # If non-significant p appear, run in loop
      if (max(p_values)>0.05) {
        repeat {
          # Get significant features, rerun model and write over model with new model
          significant_feature<-names(p_values[p_values<0.05])
          string_features<-as.formula(paste(target_col, "~", paste(significant_feature,collapse="+")))
          logistic_model[[i]]<-glm(string_features,data=df[index,],family=binomial,weights=ifelse(df[[target_col]][index]== positive, positive_weight, negative_weight))
      
          # write over coefficient with important coefficient
          coeffecient<-summary(logistic_model[[i]])$coefficients
     
          # write over p-value with important p
          p_values<-coeffecient[-1,"Pr(>|z|)"]
      
          # if all p are good, end loop 
          if (max(p_values)<0.05) break
              
          # If all p-values are NA, end loop and send warning
          if (all(is.na(p_values))) {
            cat("All p-values are NA, this model is not working")
            break
          }
        }
      }
    }
  }
  # For index create for only one model, only generate one model
  if (is.numeric(train_index)) {
    # Initialize Index of Training
    index<-train_index
    # Initialize Model
    logistic_model<-glm(as.formula(paste(target_col, "~ .")), data=df[index,], family=binomial, weights = ifelse(df[[target_col]][index] == positive, positive_weight, negative_weight))
    # Initialize Coefficient
    coeffecient<-summary(logistic_model)$coefficients
    # Initialize p-value list
    p_values<-coeffecient[-1,"Pr(>|z|)"]
    
    # If non-significant p appear, run in loop
    if (max(p_values)>0.05) {
      repeat {
        # Get significant features, rerun model and write over model with new model
        significant_feature<-names(p_values[p_values<0.05])
        string_features<-as.formula(paste(target_col, "~", paste(significant_feature,collapse="+")))
        logistic_model<-glm(string_features,data=df[index,],family=binomial,weights=ifelse(df[[target_col]][index]== positive, positive_weight, negative_weight))
      
        # write over coefficient with important coefficient
        coeffecient<-summary(logistic_model)$coefficients
     
        # write over p-value with important p
        p_values<-coeffecient[-1,"Pr(>|z|)"]
      
        # if all p are good, end loop 
        if (max(p_values)<0.05) break
        # If all p-values are NA, end loop and send warning
        if (all(is.na(p_values))) {
          cat("All p-values are NA, this model is not working")
          break
          }
        }
      }
    }
  return(logistic_model)  
}
```


## Check Model's Performance with Accuracy, True Positive Rate, True Negative Rate and F1 (func)
```{r}
# Input: 1). predict_prob: the prediction result (in probability/response)
#        2). threshold: above what number is positive
#        3). positive: target's positive value 
#        4). negative: target's negative value
#        5). df: the data frame you use to make prediction
#        6). test_index: the index you select for test
#        7). target_col: your target name 
# Return:1). a list of result include accuracy, tpr, tnr, F1 value and output result
check_model_performance<-function (predict_prob, threshold, positive, negative, df, test_index, target_col) {
  
  # Make sure its not a list but a series of numerical value that can 
  # be used for comparison
  if (is.list(predict_prob)){
    predict_prob<-unlist(predict_prob)
  }
  
  # First get prediciton & real value
  prediction<-ifelse(predict_prob>=threshold,positive,negative)
  true_value<-as.numeric(as.character(df[[target_col]][test_index]))
  
  # Calculate their tn, tp, fp, fn
  TN<-sum(prediction==negative&true_value==negative)  
  TP<-sum(prediction==positive&true_value==positive)
  FP<-sum(prediction==positive&true_value==negative)
  FN<-sum(prediction==negative&true_value==positive)
  
  # Calculate their precision & recall for F1
  # If denominator is zero, call zero as result to avoid NA
  precision<-ifelse(TP+FP==0, 0, TP/(TP+FP))
  recall<-ifelse(TP+FN==0, 0, TP/(TP+FN))
  F1<-ifelse(precision+recall==0, 0, round(2*precision*recall/(precision+recall),2))
  
  # Calculate accuracy, tpr, tnr
  accuracy<-round(((TP+TN)/(TP+TN+FP+FN))*100,2)
  tpr<-round((TP/(TP+FN))*100,2)
  tnr<-round((TN/(TN+FP))*100,2)
  
  # Print out result
  cat("The accuracy is", accuracy, "%. The True Positive Rate is ", tpr, "%. And the True Negative Rate is ", tnr, "% and the F1 score is", F1,".\n")
  
  return(list(
    accuracy=accuracy,
    tpr=tpr,
    tnr=tnr,
    F1=F1))
}
```


## Cross Validation (func 1~2)
### Split train index into k stratified (func 1)
```{r}
# Input: 1). k: the number of fold you wanna test 
#        2). df: the data frame you wanna test with
#        3). target_col: the column that need to be predict
#        4). train_index: training idex that need to split in k fold
#        5). positive: target_col's positive value
#        6). negative: target_col's negative value
# Return:1). index_list: k stratified index list 
# Usage: 1). index_list<-k_stratified_cv(5,df,"col_name",1,0)
k_stratified_cv<-function(k,df,target_col,train_index,positive,negative){
  
  # Get train_index
  train_label<-df[[target_col]][train_index]
  
  # Get positive negative's index
  positive_label<-which(train_label==positive)
  negative_label<-which(train_label==negative)
  
  # Get positive negative's local position in train index
  positive_index<-train_index[positive_label]
  negative_index<-train_index[negative_label]
  
  # Initialize length of positive/negative that need to be selected
  length_sub_p<-floor(length(positive_index)/k)
  length_sub_n<-floor(length(negative_index)/k)
  
  # Initialize a list to store all index
  index_list<-vector("list",k)
  
  # Loop over k stratified
  for (i in 1:k) {
    
    # If this is the last one, include all index (so no one is left out)
    # Avoid residual problem
    if (i==k){
      index_list[[i]]<-c(positive_index,negative_index)
      break
    }
  
    # Get positive index, negative index as designated portion so it won't 
    # fall in class imbalance
    p_sub<-positive_index[1:length_sub_p]
    n_sub<-negative_index[1:length_sub_n]
  
    # combine them as index list and shuffle so it won't be positive/negative 
    # cluster together
    index_list[[i]]<-sample(c(p_sub,n_sub))
    # remove selected index
    positive_index<-positive_index[-(1:length_sub_p)]
    negative_index<-negative_index[-(1:length_sub_n)]
  }
  return(index_list)
}
```

### Check Performance with k stratified (func 2)
(need function backward_p_lr & check_model_performance to perform)

(if user want to use this cross_validation function for randomForest, C5.0, rpart, they need to library the package first)
```{r}
# Input: 1). df: the data frame you wanna test with
#        2). index_list: the index list generated earlier
#        3). positive: target_col's positive value
#        4). negative: target_col's negative value
#        5). positive_weight: the portion of positive of overall value
#        6). negative_weight: the portion of negative of overall value
#        7). target_col: the column need to be predicted
#        8). model_type: this function support "logistic regression", "random forest","c5.0","decision tree"
#        9). threshold: set to 0.5, but can be changed
# Return:Nothing, but will tell you the performance of this specific pipeline  
# Usage: cross_validation(df,index_list,1,0,1,1,"col_name","lr")
# Usage: library(randomForest)
#        cross_validation(df,index_list,1,0,1,1,"col_name","rf")
# Usage: library(C50)
#        cross_validation(df,index_list,1,0,1,1,"col_name","c5")
# Usage: library(rpart)
#        cross_validation(df,index_list,1,0,1,1,"col_name","rpart")
cross_validation<-function(df,index_list,positive,negative,positive_weight,negative_weight,target_col,model_type,threshold=0.5){
  
  # make sure target feature is factor (categorical feature)
  df[[target_col]]<-factor(df[[target_col]], levels=c(negative, positive))
  
  # Initialization for overall performance calculation
  F1_total<-0
  accuracy_total<-0
  tpr_total<-0
  tnr_total<-0
  
  # For each index list in k fold index list
  for (i in 1:length(index_list)) {
    
    # Select list i as test, rest as train
    test_index<-index_list[[i]]
    train_index<-unlist(index_list[-i])
    
    # If model_type is logistic regression, train logistic regression model with backward p
    if (tolower(model_type) %in% c("logistic regression","lr")) {
      
      # train model with train index
      model<-backward_p_lr(df,train_index,target_col,positive,positive_weight,negative_weight)
    
      # get prediction probability with test index
      predict_v<-predict(model,df[test_index,],type="response")
      
      }
    
    # If model_type is random forest, train random forest model with probability as result
    else if (tolower(model_type) %in% c("random forest","rf")) {
      
      # Train random forest model with train index
      rf_model<-randomForest(as.formula(paste(target_col, "~ .")), data=df[train_index,], ntree=500, mtry=floor(sqrt(ncol(df))), importance=TRUE)
    
      # get prediction probability with test index
      predict_v<-predict(rf_model,df[test_index,],type="prob")[,as.character(positive)]
      
    }
    
    
    # If model_type is C5.0, train C5.0 model with probability as result
    else if (tolower(model_type) %in% c("c5.0","c5","c50")) {

      # train C5.0 model with train index
      feature<-setdiff(names(df),target_col)
      c5_m<-C5.0(x=df[train_index, feature],y=df[train_index, target_col],trials=10)
      
      # get prediction probability with test index
      predict_v<-predict(c5_m, df[test_index,], type="prob")[,as.character(positive)]
      
    }
    
    # If model_type is rpart, train rpart model with probability as result
    else if (tolower(model_type) %in% c("rpart","decision tree")) {

      # train rpart model with train index
      tree_m<-rpart(as.formula(paste(target_col, "~ .")), data=df[train_index,],method="class")
      
      # get prediction probability with test index
      predict_v<-predict(tree_m, df[test_index,], type="prob")[,as.character(positive)]
      
    }
    
    # If model does not belong to any model above, stop and send suggestion
    else {
      
      stop("Only support logistic regression, random forest, c5.0, decision tree")
      
    }
    
    # output result so user can follow & not confused
    cat("Now is runnung fold",i,"\n")
    # get performance's value
    result<-check_model_performance(predict_v,threshold, positive, negative, df, test_index, target_col)
    accuracy<-result$accuracy
    tpr<-result$tpr
    tnr<-result$tnr
    F1<-result$F1
    
    # calculate overall performance's value by adding up for future average calculation
    F1_total<-F1_total+F1
    accuracy_total<-accuracy_total+accuracy
    tpr_total<-tpr_total+tpr
    tnr_total<-tnr_total+tnr
  }
  
  # get overall performance's value by calculate the mean
  F1_overall<-round(F1_total/length(index_list),2)
  accuracy_overall<-round(accuracy_total/length(index_list),2)
  tpr_overall<-round(tpr_total/length(index_list),2)
  tnr_overall<-round(tnr_total/length(index_list),2)
  
  # and output result
  cat("Overall the cross validation of this model's performance's accuracy is ",accuracy_overall,"%, true positive rate is",tpr_overall,"%, true negative rate is", tnr_overall,"% and F1 is",F1_overall,".\n")
  return(list(
    accuracy_overall=accuracy_overall,
    tpr_overall=tpr_overall,
    tnr_overall=tnr_overall,
    F1_overall=F1_overall
  ))
}
```


## Use Ensemble Model Make Prediction(func)
```{r}
# This Function Support Logistic Regression, RandomForest, C5.0 & rpart
# Input: 1). model_list: list of logistic regression model
#        2). df_list:  the list of data frame that aligned with model_list
#        3). test_index: the list of index for ensemble train
#        4). positive: target column's positive value
#        5). target_treatment: if target feature is transformed in feature transformation
# Return:1). ensemble_predictions: Mean Prediction made by number of ensemble's logistic model
make_ensemble_predict<-function (model_list,df_list,test_index,positive,target_treatment="none") {
  
  # Initialization
  prediction_list<-vector("list",length(model_list))
  
  # If the list of model only has one set of data frame
  # make replication so its a list of data frame
  if (inherits(df_list,"data.frame")) {
    df_list <- rep(list(df_list), length(model_list))
  }
  
  # send warning if model list & data frame list is not same length
  if (length(model_list) != length(df_list)) {
    stop("Model List and Data Frame List must be equal number")
  }
  
  # For each model in ensemble model, make prediction
  for (i in seq_along(model_list)) {
    model<-model_list[[i]]
    df<-df_list[[i]]
    if (inherits(model, "glm")) {
      prediction_list[[i]]<-predict(model_list[[i]], df[test_index,], type="response")
    }
    else if (inherits(model, "randomForest") || inherits(model, "C5.0") || inherits(model,"rpart")) {
      prediction_list[[i]]<-predict(model, df[test_index,], type="prob")[, as.character(positive)]
    }
    else {
      stop("Only support logistic regression, randomForest, C5.0, rpart")
    }
  }
  
  #  & calculate mean
  ensemble_predictions<-Reduce("+",prediction_list)/length(model_list)
  
  # make sure its number not list to avoid error
  ensemble_predictions <- as.numeric(ensemble_predictions)
  
  # check if transformation occured or not
  if (tolower(target_treatment)!="none") {
    ensemble_predictions<-reverse_num(ensemble_predictions)
  }
  
  return(ensemble_predictions)
}
```


## Find Best Threshold with Test Data Set (func)
```{r}
# Reminder: target must be numerical value
# Input: 1). predict_prob: the prediction result (in probability/response)
#        2). df: the data frame you use to make prediction
#        3). test_index: the index you select for test
#        4). target_col: your target name
#        5). positive: target positive value
#        6). negative: target negative value
# Return:1). best F1 value
find_best_threshold<-function (predict_prob, df,test_index, target_col, positive, negative) {
  
  # Get true value
  real_value<-as.numeric(as.character(df[[target_col]][test_index]))
  # Initialize Threshold & F1 for later use
  best_threshold<--Inf
  largest_F1<--Inf
  
  # Loop over all possible threshold
  for (i in seq(0.01,0.99,by=0.01)) {
    
    # Make prediction based on threshold i
    prediction_result<-ifelse(predict_prob >= i,positive,negative)
    
    # Calculate threshold i's F1
    TP<-sum(prediction_result==positive&real_value==positive)
    FP<-sum(prediction_result==positive&real_value==negative)
    FN<-sum(prediction_result==negative&real_value==positive)
    precision<-ifelse(TP+FP==0, 0, TP/(TP+FP))
    recall<-ifelse(TP+FN==0, 0, TP/(TP+FN))
    F1<-ifelse(precision+recall==0, 0, 2*precision*recall/(precision+recall))
    
    # If F1 larger than any previous F1
    if (!is.na(F1) && F1>largest_F1){
      
      # update F1
      largest_F1<-F1
      
      # update threshold
      best_threshold<-i
    }
  }
  
  # Output results
  cat("The best threshold is", best_threshold, "and it gives largest F1",round(largest_F1,3),".")
  return(best_threshold)
}
```


## Get Each Ensemble Model Weight with F1 (func)
```{r}
# Input: 1). model_list: the list of model, support logistic regression, C5.0, rpart, random forest
#        2). df_list: the list of data frame align with model_list
#        3). test_index: the index of 25% testing
#        4). best_threshold: the best threshold you get after run function "find_best_threshold"
#        5). target_col: the target column that need to make prediction
#        6). positive: target positive value
#        7). negative: target negative value
# Return:1). Each Ensemble Model's Weight
ensemble_weight_F1<-function(model_list, df_list,test_index, best_threshold, target_col, positive, negative) {
  
  # Initialize a list to store F1 value
  F1_list<-list()
  
  # Initialization
  prediction_list<-vector("list",length(model_list))
  
  # If the list of model only has one set of data frame
  # make replication so its a list of data frame
  if (inherits(df_list,"data.frame")) {
    df_list <- rep(list(df_list), length(model_list))
  }
  
  # send warning if model list & data frame list is not same length
  if (length(model_list) != length(df_list)) {
    stop("Model List and Data Frame List must be equal number")
  }
  
  # Loop over all ensemble model
  for (i in 1:length(prediction_list)) {
    model<-model_list[[i]]
    df<-df_list[[i]]
    
    # if is logistic regression, make prediction with type response
    # and get its model type for output result
    if (inherits(model, "glm")) {
      prediction_list[[i]]<-predict(model_list[[i]], df[test_index,], type="response")
      model_type<-"Logistic Regression"
    }
    
    # if belongs to tree prediction, use probability make prediction and
    else if (inherits(model, "randomForest") || inherits(model, "C5.0") || inherits(model,"rpart")) {
      prediction_list[[i]]<-predict(model, df[test_index,], type="prob")[, as.character(positive)]
      
      # get their model type for output result
      if (inherits(model,"randomForest")) {
        model_type<-"Random Forest"
      }
      else if (inherits(model,"C5.0")) {
        model_type<-"C5.0"
      }
      else if (inherits(model,"rpart")) {
        model_type<-"Decision Tree"
      }
    }
    
    # if other model type appeared, halt function and send suggestion
    else {
      stop("Only support logistic regression, randomForest, C5.0, rpart")
    }
  
  # get prediction value with best threshold & its true value
  prediction<-ifelse(prediction_list[[i]] >= best_threshold,positive,negative)
  real_value<-as.numeric(as.character(df[[target_col]][test_index]))
    
  # Calculate F1 with true value and predicted value and get it into list
  TP<-sum(prediction==positive&real_value==positive)
  FP<-sum(prediction==positive&real_value==negative)
  FN<-sum(prediction==negative&real_value==positive)
  precision<-TP/(TP+FP)
  recall<-TP/(TP+FN)
  F1<-2*(precision*recall)/(precision+recall)
  F1_list[[i]]<-F1
  
  # out put result
  cat("The model",model_type, i,"'s F1 is",round(F1_list[[i]],3),"\n")
  }
  
  # Calculate weight and get it into list
  F1_vector<-unlist(F1_list)
  F1_weights<-F1_vector/sum(F1_vector)
  weight_list<-as.list(F1_weights)
  
  # output weight result
  cat("Each of their weight are",round(as.numeric(weight_list),3),".")
  
  return(weight_list)
}
```


## Use Weight Make Prediction with Ensemble Model (func)
```{r}
# Support Logistic Regression, random forest, C5.0, rpart
# Input: 1). model_list: the logistic regression model you create with your ensemble list
#        2). df: the data frame you use to make prediction
#        3). index: the index you wanna try with this model (usually test & val index)
#        4). weight_list: the weight you get for each ensemble
#        5). postive: target's positive value
# Return:1). ensemble_predictions: list of prediction (in probability) made 
#            with ensemble Logistic Regression model
ensemble_result_with_weight<-function(model_list,df_list,index,weight_list,positive,target_treatment="none") {
  
  # If the list of model only has one set of data frame
  # make replication so its a list of data frame
  if (inherits(df_list,"data.frame")) {
    df_list <- rep(list(df_list), length(model_list))
  }
  
  # Check if model, weight and data frame are in same length, sending warning if not
  if (length(model_list) != length(weight_list)) {
    stop("Model List and Weight List must be equal number")
  }
  else if (length(model_list) != length(df_list)) {
    stop("Model List and Data Frame List must be equal number")
  }
  
  # Initialization
  prediction_list<-vector("list",length(model_list))
  
  # For each model in ensemble model, make prediction
  # if is logistic regression, make prediction with response
  # if is tree prediction, make prediction with probability
  # if other model appeared, halt function & send suggestion
  for (i in seq_along(model_list)) {
    model<-model_list[[i]]
    df<-df_list[[i]]
    if (inherits(model, "glm")) {
      prediction_list[[i]]<-predict(model, df[index,], type="response")
      prediction_list[[i]]<-prediction_list[[i]]*weight_list[[i]]
    }
    else if (inherits(model, "randomForest") || inherits(model, "C5.0") || inherits(model,"rpart")) {
      prediction_list[[i]]<-predict(model, df[index,], type="prob")[, as.character(positive)]
      prediction_list[[i]]<-prediction_list[[i]]*weight_list[[i]]
    }
    else {
      stop("Only support logistic regression, randomForest, C5.0, rpart")
    }
  }
  
  # Add up as final prediction value
  ensemble_predictions<-Reduce("+",prediction_list)
  
  # Calculate ensemble model's mean as final prediciton value
  if (tolower(target_treatment)!="none") {
    ensemble_predictions<-reverse_num(ensemble_predictions,target_treatment)
  }
  
  return(ensemble_predictions)
}
```


## Stack Model for Categorical Prediction (with glm) (func 1~3)
### Helper Function: generate a data frame that's ready for stack model training (func 1)
(Remind user we only support logistic regression, random forest, C5.0 & rpart model)
```{r}
# This is a helper function designed for stack_model_lr & stack_model_predict_lr
# The purpose of this function is to generate a data frame that's ready to train
# or run a stack model
# Input: 1). model_list: a list of model, support logistic regression, 
#            random forest, C5.0 & rpart
#        2). df_list: a list of data frame that aligned with model list
#        3). index: the index you wanna use to train this model, could be test or validation
#        4). positive: the positive value of your target
# Return:1). a data frame that ready for stack model
generate_stack_df<-function(model_list, df_list, index, positive) {
  
  # Initialization
  stack_df<-data.frame(matrix(nrow = length(index), ncol = 0))
  
  # Initialization
  prediction_list<-list()
  
  # If the list of model only has one set of data frame
  # make replication so its a list of data frame
  if (inherits(df_list,"data.frame")) {
    df_list <- rep(list(df_list), length(model_list))
  }
  
  # send warning if model list & data frame list is not same length
  if (length(model_list) != length(df_list)) {
    stop("Model List and Data Frame List must be equal number")
  }
  
  # Loop over all ensemble model
  for (i in 1:length(model_list)) {
    model<-model_list[[i]]
    df<-df_list[[i]]
    
    # if is logistic regression, make prediction with type response
    if (inherits(model, "glm")) {
      prediction_list[[i]]<-predict(model, df[index,], type="response")
    }
    
    # if belongs to tree prediction, use probability make prediction
    else if (inherits(model, "randomForest") || inherits(model, "C5.0") || inherits(model,"rpart")) {
      prediction_list[[i]]<-predict(model, df[index,], type="prob")[, as.character(positive)]
    }
    
    # if other model type appeared, halt function and send suggestion
    else {
      stop("Only support logistic regression, randomForest, C5.0, rpart")
    }
    # paste prediction value into prediction list sequentially
    stack_df[[paste0("model",i)]]<-prediction_list[[i]]
  }
  return(stack_df)
}
```

### Build Stack Model with glm (func 2)
(Need function generate_stack_df to perform)
```{r}
# Purpose is use a list of model to train a stack model 
# Support logistic regression, random forest, C5.0 & rpart
# Input: 1). model_list: a list of model, support logistic regression, 
#            random forest, C5.0 & rpart
#        2). df_list: a list of data frame that aligned with model list
#        3). test_index: the index you use prepare to tune model
#        4). positive: the positive value of your target
#        5). target_col: the feature you wanna make prediction
# Return:1). stack_model: a stack model made with a list of model
stack_model_lr<-function(model_list,df_list,test_index,positive,target_col){
  
  # Generate data frame suitable for stack
  stack_df<-generate_stack_df(model_list,df_list,test_index, positive)
  # Get real value
  true_value<-as.numeric(as.character(df_list[[1]][[target_col]][test_index]))
  
  # Use real value & prediciton value build new logistic regression model
  stack_model<-glm(true_value~., data=stack_df, family=binomial)
  
  # as a stack model
  return (stack_model)
}
```

### Test Stack Model with Your Data Set (func 3)
(Need function generate_stack_df & check_model_performance to perform)

(Use test to tune threshold & make final prediction)
```{r}
# This function's purpose is to use the stack model you build earlier to make
# prediction
# Input: 1). stack_model: the stack_model you build earlier
#        2). model_list: a list of model, support logistic regression, 
#            random forest, C5.0 & rpart
#        3). df_list: a list of data frame that aligned with model list
#        4). index: the index you wanna test this model, could be test & validation
#        5). positive: the positive value of your target
#        6). negative: the negative value of your target
#        7). target_col: the feature you wanna make prediction
#        8). threshold=0.5: I set to reasonable 0.5, but you can change this 
#            after threshold tune
# Return:1). Output the overall performance of stack model
#        2). prediction_value: your prediction value for check purpose & threshold tune
#        3). real_vaue: the true value for checking purpose & threshold tune
stack_model_predict_lr<-function(stack_model,model_list,df_list,index,positive,negative,target_col,threshold=0.5){
  
  # Generate a data frame that suit stack model's purpose
  stack_df<-generate_stack_df(model_list,df_list,index, positive)
  
  # Get real value
  true_value<-as.numeric(as.character(df_list[[1]][[target_col]][index]))
  
  # Use prediction value & stack model make new prediction
  prediction_value<-predict(stack_model,newdata=stack_df,type="response")
  
  # Output performance of stack model
  result<-check_model_performance(prediction_value,threshold,positive,negative,df_list[[1]],index,target_col)
  
  # as a list
  return (list(
    prediction_value=prediction_value,
    true_value=true_value,
    accuracy=result$accuracy,
    tpr=result$tpr,
    tnr=result$tnr,
    F1=result$F1
  ))
}
```


# **3. Data Cleaning**


## Load Data Set
```{r}
df<-read.csv(file = "https://docs.google.com/spreadsheets/d/1OZIF-LjbtDOWoW2o90ZXtN_I_lBO3d0tE3t3yUlchsA/export?format=csv",
             header=TRUE,
             stringsAsFactors = TRUE)
```


## Check Data Set: Head
(Features like FastingBS & HeartDisease should be considered as categorical feature which now is numerical feature)
```{r}
head(df,5)
```


## Check Data Set: Summary
(Oldpeak has negative value need a right shift)
```{r}
summary(df)
```


## Change FastingBS (Fasting Blood Sugar) & Heartdisease into Categorical Feature
(Fasting Blood Sugar is 1 means abnormal and 0 means normal, its not a continuous value therefore it should be considered as categorical feature)

(Heardiease 1 means True 0 means False, also need to be considered as categorical feature)
```{r}
df$FastingBS<-as.factor(df$FastingBS)
df$HeartDisease<-as.factor(df$HeartDisease)
```


## Perform a Right Shift for Oldpeaks 
(Oldpeaks has negative values, perform a right shift so its ready for later feature engineering)
```{r}
# Get minimum value and perform right shift
op_min<-min(df$Oldpeak)
df$Oldpeak<-df$Oldpeak+(abs(op_min))
```


## Initialize a Data Set for Random Forest
(I think random forest's data set does not need outlier treatment because random forest is built by using bag making decision, its not very sensitive to outliers & "outliers" can help random forest capture features.But feature Cholesterol need kNN imputation which requires outlier treatment in the begining. Therefore I initialize another data set for Random Forest so I can impute NA treated cloumns after NA treatment)
```{r}
df_RF<-df
```


## Check How Many and if NA & zero Occur in Data Frame
(func usage: check_na_zero)
```{r}
check_na_zero(df)
```


## Zero into NA
(func usage: replace_na_with_zero)

(Resting Blood Pressure's zero value should be considered as NA because living person's resting blood pressure should all above zero)

(Cholesterol's zero value should be considered as NA because no living person's Cholesterol is zero)

(Oldpeaks's zero/negative value are normal because it calculated by using resting ST minus exercise ST. Therefore zero only represent resting ST & exercise ST is the same; negative refer resting ST is less than exercise ST.)
```{r}
df<-replace_na_with_zero(df,ignore_cols = "Oldpeak")
```


## Check NA
(func usage: check_na_zero)

(Check if designated zero vaue are changed into NA and it does)
```{r}
check_na_zero(df)
```


## Check Column with Missing Value's Distribution
(Different feature's NA value should be treated differently based on their distribution. Normally distributed feature with few NA can use median imputation, Like resting blood pressure; for Cholesterol, even its also normally distributed when disregard NA, because it has large portion of NA value, I choose to use kNN imputation to introduce randomness into this data set so it won't introduce bias.)
```{r}
hist(df$RestingBP,prob=TRUE, main="Histogram of Resting Blood Pressure", xlab="Resting Blood Pressure")
lines(density(df$RestingBP, na.rm=TRUE))
```

```{r}
hist(df$Cholesterol,prob=TRUE, main="Histogram of Cholesterol", xlab="Cholesterol")
lines(density(df$Cholesterol, na.rm=TRUE))
```


## Median Imputation for Resting Blood Pressure
(func usage: median_imputation)

(Because it only has one missing value & it's very normally distributed like)
```{r}
df<-median_imputation(df,"RestingBP")
```

### Check if NA of Resting Blood Pressure is Successfully Imputated
(func usage: check_na_zero)

(it does)
```{r}
check_na_zero(df)
```


## kNN Imputation for Cholesterol
(Because there is great portion of missing value in Cholesterol, I think median imputation may introduce bias therefore I produce kNN Imputation)

### Check Numerical Value's Distribution that will be used for kNN Imputation
(For kNN imputation, we need outlier treatment for numerical features that will be used for kNN imputation so they won't introduce bias)

(Different feature should treat their outliers differently based on their distribution. For normally distributed feature we can use z-score treatment, for not normally distributed feature, IQR is recommended.)
```{r}
hist(df$Age,prob=TRUE, main="Histogram of Age", xlab="Age")
lines(density(df$Age, na.rm=TRUE))
```


```{r}
hist(df$RestingBP,prob=TRUE, main="Histogram of Resting Blood Pressure", xlab="Resting Blood Pressure")
lines(density(df$RestingBP, na.rm=TRUE))
```


```{r}
hist(df$MaxHR,prob=TRUE, main="Histogram of Maximum Heart Rate", xlab="Maximum Heart Rate")
lines(density(df$MaxHR, na.rm=TRUE))
```


```{r}
hist(df$Oldpeak,prob=TRUE, main="Histogram of Oldpeak", xlab="Oldpeak")
lines(density(df$Oldpeak, na.rm=TRUE))
```


### Outlier Treatment
(func usage: z_score_outlier)

(If normally distributed: z-score outlier treatment; if not normally distributed: IQR treatment)

(Because they are all normally distributed like features, I am gonna perform z-score outlier treatment for all these features)
```{r}
df<-z_score_outlier(df,c("Age","RestingBP","MaxHR","Oldpeak"))
```

### Check if Outliers Successfully Treated
(it does)
```{r}
summary(df)
```

### kNN Imputation for Feature Cholesterol
(func usage: initialize_distance_find_best_k, initialize_not_na_index, initialize_test_k_index, find_best_k, kNN_Imputation, automation_knn_imputation)
```{r}
set.seed(888)
df<-automation_knn_imputation(df,test_por=0.1,max_k=30)
```

### Check if NA is Successfully Imputated
(func usage: check_na_zero)

(it does)
```{r}
check_na_zero((df))
```


## Prepare Data Set for Random Forest with NA Treated Features
(Now with all NA successfully treated, random forest has its designated data set. I replace random forest's data set's feature that has NA value with successfully NA treated feature value.)
```{r}
df_RF$RestingBP<-df$RestingBP
df_RF$Cholesterol<-df$Cholesterol
```

### Check Random Forest Data Set is Okay
(func usage: check_na_zero)

(look good)
```{r}
check_na_zero(df_RF)
```


## Check Newly NA Imputated Feature's Distribution for Outlier Treatment
(Still, check if they are normally distributed or not so they can treated differently)

(We notice Cholesterol looks a little skewed. Therefore I am gonna perform IQR treatment for Cholesterol)
```{r}
hist(df$Cholesterol,prob=TRUE, main="Histogram of Cholesterol", xlab="Cholesterol")
lines(density(df$Cholesterol, na.rm=TRUE))
```

### IQR Outlier Treatment for Cholesterol
(func usage: IQR_outlier)

(Cause this feature is a little right skewed)
```{r}
df<-IQR_outlier(df,"Cholesterol")
```


### Check if Outliers Successfully Treated
(it does)
```{r}
summary(df)
```

## Prepare Data Set for Logistic Regression
(For Logistic Regression Model, it requires transformation, one-hot encoding, etc and we want to perform all these in a separate data set. Therefore I create another data set for logistic regression)
```{r}
df_LR<-df
```

### Check if Logistic Regression Data Frame has all Features we Need
(yes it does)
```{r}
summary(df_LR)
```


## Prepare Data Set for C5.0 Model
(With all NA & Outliers been Treated, we have all features to train a C5.0 model, therefore I generate separate data set for this model)

(C5.0 is sensitive to extreme outliers comparing to random forest because random forest use bagging therefore outlier may not be chosen every time which eliminate its influence. However, C5.0 is just a single tree, extreme outliers will influence how it makes decision.)
```{r}
df_C5<-df
```

### Check if C5.0 Model's Data Set has All Features we Need
(it does)
```{r}
head(df_C5,5)
```



# **4. Logistic Regression Model**


## *4.1. Feature Engineering (LR)*

### Feature Engineering: New Derived Feature (MaxHR * Oldpeak)
(Max Heart Rate means the rate of heart beat after exercise, the higher the more dangerous. Oldpeak means how heart is desparate for blood after exercise, the higher the riskier. I combined both feature as new feature because when both situations occur (both MaxHR & Oldpeak increase), patient is more likely become a potential patient)
```{r}
# Create New Feature Double Risk by MaxHR & Oldpeak
df_LR$doubleRISK<-df_LR$MaxHR * df_LR$Oldpeak
```

### Feature Engineering: Check Distribution of Numerical Features
(Check if there are feature not normally distributed and need transformation.)
```{r}
hist(df_LR$Age,prob=TRUE, main="Histogram of Age Before", xlab="Age")
lines(density(df_LR$Age, na.rm=TRUE))
```

```{r}
hist(df_LR$RestingBP,prob=TRUE, main="Histogram of Resting Blood Pressure Before", xlab="Resting Blood Pressure")
lines(density(df_LR$RestingBP, na.rm=TRUE))
```

```{r}
hist(df_LR$Cholesterol,prob=TRUE, main="Histogram of Cholesterol Before", xlab="Cholesterol")
lines(density(df_LR$Cholesterol, na.rm=TRUE))
```

```{r}
hist(df_LR$MaxHR,prob=TRUE, main="Histogram of Maximum Heart Rate Before", xlab="Maximum Heart Rate")
lines(density(df_LR$MaxHR, na.rm=TRUE))
```

```{r}
hist(df_LR$Oldpeak,prob=TRUE, main="Histogram of Oldpeak Before", xlab="Oldpeak")
lines(density(df_LR$Oldpeak, na.rm=TRUE))
```

```{r}
hist(df_LR$doubleRISK,prob=TRUE, main="Histogram of Double Risk Before", xlab="Double Risk")
lines(density(df_LR$doubleRISK, na.rm=TRUE))
```

### Feature Engineering: Perform Transformation
(Age & Oldpeak are left skewed so I perform square transformation)

(New Derived Feature DoubleRISK is a little right skewed so I perform sqrt transformation)

(When performing square, smaller value will have small increase, larger value will have large increase which manually create a right skewed, visually look more normally distributed. Similar for square root, larger value will have more decrease smaller value will have less decrease, which manually create a left skewed & visually more normally distributed)

(I didn't perform transformation on other feature because I think they are already well distributed.)
```{r}
df_LR$Age_square<-(df_LR$Age)^2
hist(df_LR$Age_square,prob=TRUE, main="Histogram of Age After", xlab="Age")
lines(density(df_LR$Age_square, na.rm=TRUE))
```

```{r}
df_LR$Oldpeak_square<-(df_LR$Oldpeak+1)^2
hist(df_LR$Oldpeak_square,prob=TRUE, main="Histogram of Oldpeak After", xlab="Oldpeak")
lines(density(df_LR$Oldpeak_square, na.rm=TRUE))
```

```{r}
df_LR$doubleRISK_sqrt<-sqrt(df_LR$doubleRISK)
hist(df_LR$doubleRISK_sqrt,prob=TRUE, main="Histogram of Double Risk After", xlab="Double Risk")
lines(density(df_LR$doubleRISK_sqrt, na.rm=TRUE))
```


### Feature Engineering: Normalization Numerical Feature
(Normalize all numerical feature so model is more interpretable and each feature is comparable)
```{r}
# Normalization all numerical feature
# Binary categorical features are transformed into factor before
numerical_var<-which(sapply(df_LR, is.numeric))
df_LR[numerical_var]<-scale(df_LR[numerical_var])
```

### Feature Engineering: Check Multicollinearity
(func usage: check_multicollinearity)

(check if there are multicollinearity occur among these numerical features because if multicollinearity occur, model cannot confidently tell which feature make contribution which will introduce bias.)
```{r}
variables<-df_LR[,c("Age_square","RestingBP","Cholesterol","MaxHR","Oldpeak_square","doubleRISK_sqrt")]
check_multicollinearity(variables)
```

### Feature Engineering: PCA
(PCA's purpose is to maximum variance. This is important cause high variance means a feature behave differently which further indicate "seperation". Low Variance, on the contrary, indicate feature are all cluster together, further indicate feature is "un-separable". In result, PC1 is the one with highest variance, PC2 is the one vertical to PC1, PC3 is vertical to PC1 & PC2, etc, which keep each feature's uniqueness as much as possible. This way, PCA bring new feature (with most unique information) from old feature, realize the purpose of reduce dimension.)

(From the result we know PC1 contribute ~32% of total variace, PC2 contribute ~27%, PC3 contribute ~16.8%, etc. PC1~PC5 already explain 99% variance)
```{r}
pca_results<-prcomp(df_LR[,c("Age_square","RestingBP","Cholesterol","MaxHR","Oldpeak_square","doubleRISK_sqrt")])
summary(pca_results)
```

### Feature Engineering: Replace Numerical Feature with PC1~PC5
(I replaced numerical feature with PC1~PC5 because like mentioned above, PC1~PC5 already explain 99% variance)

(see line 1792)
```{r}
pca_df<-as.data.frame(pca_results$x[,1:5])
```

### Feature Engineering: One-Hot Encoding Categorical Feature that has More than 2 Choices
(One-Hot encoding is good for feature with more than 2 choices and is not orginal)
```{r}
# One-Hot encode ChestPainType, RestingECG, ST_Slope
onehot_features<-model.matrix(~ChestPainType+RestingECG+ST_Slope,data=df_LR)
onehot_df<-as.data.frame(onehot_features)[,-1]
```

### Feature Engineering: zero/one Encode Binary Feature
(zero/one Encode sex, ExerciseAngina because they only have 2 features and not ordinal)
```{r}
df_LR$Sex<-ifelse(df_LR$Sex=="M", 1, 0)
df_LR$ExerciseAngina<-ifelse(df_LR$ExerciseAngina=="Y", 1, 0)
```

### Feature Engineering: Convert FastingBS Back to Numerical Feature
(FastingBS is already one/zero encoded, and should considered as numerical feature in logistic regression model construction)
```{r}
df_LR$FastingBS<-as.numeric(as.character(df_LR$FastingBS))
```

### Feature Engineering: Combine Treated Numerical Feature & Treated Categorical Feature into New Data Frame
(Create a seperate data set than contain all features ready for logistic regression model construction)
```{r}
df_encoded_LR<-cbind(df_LR[,c("Sex","FastingBS","ExerciseAngina","HeartDisease")],onehot_df,pca_df)
```

### Feature Engineering: Check if New Data Frame has all Feature We Need
(it does)
```{r}
summary(df_encoded_LR)
```


## *4.2. Modeling (LR)*


### Data Partition: Partition 3 Data Set with Portion of 50%, 25%, 25% Data Set
(func usage: three_set_partition)

(I partition data set into 3 sets: 50% for training & cross validation, 25% for testing & tuning 25% for final validation to test model performance)
```{r}
set.seed(888)
partition_result<-three_set_partition(df_encoded_LR,"HeartDisease",0.5,0.25,1,0)
train_index<-partition_result$train_index
test_index<-partition_result$test_index
validation_index<-partition_result$validation_index
```


### Data Partition: Check Index's Class Imbalance 
(func usage: check_class_imbalance)

(class imbalance will largely influence model's performance if one category is cluster together, but this won't happen because I use stratified split in partition. This output result is for double check.)
```{r}
check_class_imbalance(df_encoded_LR,train_index,"HeartDisease",1,0)
check_class_imbalance(df_encoded_LR,test_index,"HeartDisease",1,0)
check_class_imbalance(df_encoded_LR,validation_index,"HeartDisease",1,0)
```

### Cross Validation: Stratified 5 Train Index
(func usage: k_stratified_cv)

(Split train data set into 5 fold (with stratified split) so all data in train data set are included in pipeline testing, therefore the evaluation of model performing is unbiased & reliable)
```{r}
set.seed(888)
index_list<-k_stratified_cv(5,df_encoded_LR,"HeartDisease",train_index,1,0)
```

### Cross Validation: Perform Cross Validation to Check Pipeline (Logistic Regression with threshold 0.5)
(func usage: cross_validation)

(Cross Validation gives us the expected model performance so it won't induce bias. We notice there are not much difference, pipeline is valid and ready for next step.)
```{r}
# Positive & Negative are in similar portion, therefore their weights are all 1
set.seed(888)
positive_weight<-1
negative_weight<-1
lr_cv<-cross_validation(df_encoded_LR,index_list,1,0,positive_weight,negative_weight,"HeartDisease","lr")
```

```{r echo=FALSE}
# Collect information for model evaluation's table construction
# Didn't echo this chunk cause it only serves to collect information
lr_cv_a<-lr_cv$accuracy_overall
lr_cv_tpr<-lr_cv$tpr_overall
lr_cv_tnr<-lr_cv$tnr_overall
lr_cv_F1<-lr_cv$F1_overall
```

### Ensemble Logistic Regression Model: Create 10 Ensemble Index List with 80% of Total Train Index
(func usgae: ensemble_train_partition)

(I choose 80% because this is a small data set. Same size but different index.)
```{r}
train_partitions_index<-ensemble_train_partition(df_encoded_LR,train_index,10,0.8,"HeartDisease",1,0)
```

### Ensemble Logistic Regression Model: Check if There is Class Imbalance Happened
(func usgae: check_class_imbalance)

(Double check if class imbalance happened in these ensemble data set to avoid bias. It does not.)
```{r}
check_class_imbalance(df_encoded_LR,train_partitions_index,"HeartDisease",1,0)
```

### Build Bagged Logistic Regression Model: Use 10 Ensemble Set Make 10 Logistic Regression Model with Backward p
(func usgae: backward_p_lr)

(We notice this data set's negative & positive are very balanced, therefore positive & negative weight can both be 1)

(Use 10 sets of index build 10 logistic regression model)
```{r}
positive_weight<-1
negative_weight<-1
logistic_model_list<-backward_p_lr(df_encoded_LR,train_partitions_index,"HeartDisease",1,positive_weight,negative_weight)
```

### Build Bagged Logistic Regression Model: Use 10 Ensemble Model to Make a Ensemble Prediction with their Mean
(func usage: make_ensemble_predict)

(Use logistic regression's data frame make logistic regression prediction with 10 models and generate final prediction with their mean)
```{r}
ensemble_predictions<-make_ensemble_predict(logistic_model_list,df_encoded_LR,test_index,1)
```

### Build Bagged Logistic Regression Model: Test Ensemble Model's Performance
(func usage: check_model_performance)

(Check how the model perform with 0.5 as threshold. We notice not much difference with cross validation's result.)
```{r}
result_ensemble<-check_model_performance(ensemble_predictions,0.5,1,0,df_encoded_LR,test_index,"HeartDisease")
```

```{r echo=FALSE}
# collect information for model evaluation model construction
# Didn't echo this chunk cause it only serves to collect information
bag_lr_bt_a<-result_ensemble$accuracy
bag_lr_bt_tpr<-result_ensemble$tpr
bag_lr_bt_tnr<-result_ensemble$tnr
bag_lr_bt_F1<-result_ensemble$F1
```


## *4.3. Tuning (LR)*

### Tune with Threshold & Weight: Find Best Threshold from Test Data Set
(func usage: find_best_threshold)

(the threshold 0.5 meet reasonable assumption but does not meet our model's need, so I loop over all possible threhold and find the one suit best for model)
```{r}
best_threshold<-find_best_threshold(ensemble_predictions,df_encoded_LR,test_index,"HeartDisease",1,0)
```

### Tune with Threshold & Weight: Use F1 Get Each Ensemble Model's Weight
(func usage: ensemble_weight_F1)

(calculate each ensemble model's weight based on their F1 performance for test data set)
```{r}
weight_list<-ensemble_weight_F1(logistic_model_list,df_encoded_LR,test_index,best_threshold,"HeartDisease",1,0)
```

### Tune with Threshold & Weight: Get Tuned Result with Weight & Best Threshold
(func usage: ensemble_result_with_weight)

(generate ensemble result by times each model's prediction probability value with their weight (portion))
```{r}
ensemble_predictions_val<-ensemble_result_with_weight(logistic_model_list,df_encoded_LR,validation_index,weight_list,1)
```

### Tune with Threshold & Weight: Check Tuned Model's Performance with Validation Data Set
(func usage: check_model_performance)

(Use weight & best threshold make final prediction with validation data set)

(By comparing to cross validation result, we notice their difference is not too much. This result is trust worthy.)
```{r}
result_ensemble<-check_model_performance(ensemble_predictions_val,best_threshold,1,0,df_encoded_LR,validation_index,"HeartDisease")
```

```{r echo=FALSE}
# collect information for model evaluation model construction
# Didn't echo this chunk cause it only serves to collect information
bag_lr_at_a<-result_ensemble$accuracy
bag_lr_at_tpr<-result_ensemble$tpr
bag_lr_at_tnr<-result_ensemble$tnr
bag_lr_at_F1<-result_ensemble$F1
```


# **5. Random Forest Model**



## *5.1. Modeling (Random Forest)*


### Data Check: Check Data Set
(Make sure data set has all features that will be needed for model construction)
```{r}
head(df_RF,5)
```

### Data Check: Check NA/zero
(Double check if all NA value is successfully treated)
```{r}
check_na_zero(df_RF)
```

### Cross Validation: Perform Cross Validation to Check Pipeline (Random Forest with threshold 0.5)
(func usage: cross_validation)

(Cross Validation gives us the expected model performance so it won't induce bias. We notice there are not much difference, pipeline is valid and ready for next step.)
```{r message=FALSE}
set.seed(888)
positive_weight<-1
negative_weight<-1
rf_cv<-cross_validation(df_RF,index_list,1,0,positive_weight,negative_weight,"HeartDisease","rf")
```

```{r echo=FALSE}
# collect information for model evaluation model construction
# Didn't echo this chunk cause it only serves to collect information
rf_cv_a<-rf_cv$accuracy_overall
rf_cv_tpr<-rf_cv$tpr_overall
rf_cv_tnr<-rf_cv$tnr_overall
rf_cv_F1<-rf_cv$F1_overall
```


### Model Construction: Construct Random Forest Model
```{r}
# Initialization for repeat
set.seed(888)

# Build Forest
rf_model <- randomForest(HeartDisease ~ ., data=df_RF[train_index,], 
                         ntree = 500,          
                         mtry = floor(sqrt(ncol(df_RF))),        
                         importance = TRUE)
```

### Model Construction: Make Prediction with Random Forest Model
(I use probability to make prediction so its performance can be checked with my function, check_model_performance)
```{r}
prediction<-predict(rf_model,df_RF[test_index,],type="prob")[,2]
```

### Model Construction: Check Random Forest Models Performance
(func usage: check_model_performance)

(Check how model perform with threshold 0.5, better than bag logistic regression & not much difference with cross validation. Therefore this result is trustworthy.)
```{r}
result<-check_model_performance(prediction,0.5,1,0,df_RF,test_index,"HeartDisease")
```

```{r echo=FALSE}
# collect information for model evaluation model construction
# Didn't echo this chunk cause it only serves to collect information
rf_bt_a<-result$accuracy
rf_bt_tpr<-result$tpr
rf_bt_tnr<-result$tnr
rf_bt_F1<-result$F1
```


## *5.2. Tuning (Random Forest)*


### Tune Random Forest Model with Best Threshold: Find Best Threshold for Random Forest Model
(func usage: find_best_threshold)
```{r}
best_threshold<-find_best_threshold(prediction,df_RF,test_index,"HeartDisease",1,0)
```

### Tune Random Forest Model with Best Threshold: Use Best Threshold to Check Model Performance with Validation Index
(func usage: check_model_performance)

(Use best threshold make final prediction with validation data set. Comparing with prediction made with test data set, each performance feature drop a little bit but is very similar to cross validation's result. The result is trustworthy.)
```{r}
prediction<-predict(rf_model,df_RF[validation_index,],type="prob")[,2]
result<-check_model_performance(prediction,best_threshold,1,0,df_RF,validation_index,"HeartDisease")
```

```{r echo=FALSE}
# collect information for model evaluation model construction
# Didn't echo this chunk cause it only serves to collect information
rf_at_a<-result$accuracy
rf_at_tpr<-result$tpr
rf_at_tnr<-result$tnr
rf_at_F1<-result$F1
```


# **6. C5.0 Model**


## *6.1. Modeling (C5.0)*


### Data Check: Check Data Set
(Check if this data set has all features we need and it does)
```{r}
head(df_C5,5)
```

### Data Check: Check NA/zero
(Double check is all NA value are successfully impute)
```{r}
check_na_zero(df_C5)
```

### Cross Validation: Perform Cross Validation to Check Pipeline (C5.0 with threshold 0.5)
(func usage: cross_validation)

(Cross Validation gives us the expected model performance so it won't induce bias. We notice there are not much difference, pipeline is valid and ready for next step.)
```{r}
set.seed(888)
positive_weight<-1
negative_weight<-1
c5_cv<-cross_validation(df_C5,index_list,1,0,positive_weight,negative_weight,"HeartDisease","c5")
```

```{r echo=FALSE}
# collect information for model evaluation table construction
# Didn't echo this chunk cause it only serves to collect information
c5_cv_a<-c5_cv$accuracy_overall
c5_cv_tpr<-c5_cv$tpr_overall
c5_cv_tnr<-c5_cv$tnr_overall
c5_cv_F1<-c5_cv$F1_overall
```

### Model Construction: Construct a C5.0 Model
(Construct 10 trials of C5.0 model, in summary it consider sex, maxHR, exerciseAngina, ST_slope as important feature and the inaccurate prediction is only 4.1%)
```{r}
set.seed(888)
target_col<-"HeartDisease"
feature<-setdiff(names(df_C5),target_col)
c5_m<-C5.0(x=df_C5[train_index, feature],y=df_C5[train_index, target_col],trials=10,rules=TRUE)
```

### Model Construction: Use C5.0 Model Make a Prediction with Probability
(Make probability prediction so it can be used with my function check_model_performance & find_best_threshold & later ensemble)
```{r}
predict_v<-predict(c5_m, df_C5[test_index,], type="prob")[,"1"]
```

### Model Construction: Check Model's Performance with Test Index
(Check how model perform with 0.5 as threshold, not much difference with cross validation's result therefore result is valid & trustworthy)
```{r}
result<-check_model_performance(predict_v,0.5,1,0,df_C5,test_index,"HeartDisease")
```

```{r echo=FALSE}
# collect information for model evaluation table construction
# Didn't echo this chunk cause it only serves to collect information
c5_bt_a<-result$accuracy
c5_bt_tpr<-result$tpr
c5_bt_tnr<-result$tnr
c5_bt_F1<-result$F1
```


## *6.2. Tuning (C5.0)*


### Tune with Best Threshold: Find Best Threshold for C5.0 Model
(func usage: find_best_threshold)

(Threshold 0.5 meet reasonable assumption but does not meet model's need, check if can help model perform better)
```{r}
best_threshold<-find_best_threshold(predict_v,df_C5,test_index,"HeartDisease",1,0)
```

### Tune with Best Threshold: Use Best Threshold to Make Final Prediction with Validation Data Set
(func usage: check_model_performance)

(Use validation data set to make final prediction with best threshold. The model's performance drop a little when comparing to test index's prediction but still similar to cross validation therefore we are safe to say the result is valid & trustworthy)
```{r}
predict_v<-predict(c5_m, df_C5[validation_index,], type="prob")[,"1"]
result<-check_model_performance(predict_v,best_threshold,1,0,df_C5,validation_index,"HeartDisease")
```

```{r echo=FALSE}
# collect information for model evaluation table construction
# Didn't echo this chunk cause it only serves to collect information
c5_at_a<-result$accuracy
c5_at_tpr<-result$tpr
c5_at_tnr<-result$tnr
c5_at_F1<-result$F1
```


# **7. Ensembling 1 (Bag) (Logistic Regression, Random Forest, C5.0)**


## *7.1. Modeling (Final Ensemble 1)*


### Modeling: Initialization
(Create a list of model & a list of data frame that aligned with model list so it can run in my function make_ensemble_predict)
```{r}
# Generate the list of data frame for bagged logistic regression & Random Forest & C5.0
df_list_lr<-rep(list(df_encoded_LR),length(logistic_model_list))
df_list<-c(df_list_lr,list(df_RF),list(df_C5))
# Generate the list of model
model_list<-c(logistic_model_list, list(rf_model), list(c5_m))
```

### Modeling: Make Ensemble Prediction with Test Index
(func usage: make_ensemble_predict)

(make ensemble preiction with function make_ensemble_predict by calculate their mean. data frame list need to align with model list so prediction can be valid)
```{r}
ensemble_predictions<-make_ensemble_predict(model_list,df_list,test_index,1)
```

### Modeling: Check Performance
(func usage: check_model_performance)

(Check ensemble model's performance with 0.5 as threshold. Very similar result compare to bag logistic regression, random forest & C5.0)
```{r}
result<-check_model_performance(ensemble_predictions,0.5,1,0,df_C5,test_index,"HeartDisease")
```

```{r echo=FALSE}
# collect information for model evaluation's table construction
# Didn't echo this chunk cause it only serves to collect information
e1_bt_a<-result$accuracy
e1_bt_tpr<-result$tpr
e1_bt_tnr<-result$tnr
e1_bt_F1<-result$F1
```


## *7.2. Tuning (Final Ensemble 1)*


### Tuning with Best Threshold & Weight: Find Best Threshold
(Find the best threshold for ensemble model with test data set. Use any data set is fine cause we are only interested in target's information)
```{r}
best_threshold<-find_best_threshold(ensemble_predictions, df_C5,test_index, "HeartDisease", 1, 0)
```

### Tuning with Best Threshold & Weight: Calculate Weight Based on F1
(Model with higher F1 should contribute more to overall prediction & model with less F1 should contribute less to overall prediction)

(Calculate weight in portion to learn how each model should contribute to overall prediciton)
```{r}
weight_list<-ensemble_weight_F1(model_list, df_list,test_index, best_threshold, "HeartDisease", 1, 0)
```


## *7.3. Final Validation (Final Ensemble 1)*


### Final Validation: Make Prediction with Weight & Validation Index
(Use best threshold, weight to make final prediction with validation data set)

(model list & data frame list are built before, I don't want to regenerate this to mess things up because weight list is calculate in this sequence)
```{r}
ensemble_predictions_val<-ensemble_result_with_weight(model_list,df_list,validation_index,weight_list,1)
```

### Final Validation: Check Performance with Best Threshold
(Check how our model performed using best threshold & weight list with validation data set. There is a small drop of features other than true positive rate but I think its acceptable because its more important to capture all potential patients therefore a little sacrifice for tnr is okay if we could boost tpr)
```{r}
result<-check_model_performance(ensemble_predictions_val,best_threshold,1,0,df_C5,validation_index,"HeartDisease")
```

```{r echo=FALSE}
# collect information for model evaluation's table construction
# Didn't echo this chunk cause it only serves to collect information
e1_at_a<-result$accuracy
e1_at_tpr<-result$tpr
e1_at_tnr<-result$tnr
e1_at_F1<-result$F1
```


## *7.4. Use Ensemble 1 Make Prediction*
### Make Prediction with First Patient as Example
(Use A List of Model Make Actual Prediciton with a Single Patient)

(This serve to provide an example of how users can use this function to make actual prediction)
```{r}
# Use third patient as example, make a list of data frame align with list of model
df_LR_patient<-df_encoded_LR[validation_index[3],]
df_RF_patient<-df_RF[validation_index[3],]
df_C5_patient<-df_C5[validation_index[3],]
df_list_patient<-c(rep(list(df_LR_patient),length(logistic_model_list)),list(df_RF_patient),list(df_C5_patient))

# Make Prediciton with our ensemble model
ensemble_predictions<-ensemble_result_with_weight(model_list,df_list_patient,1,weight_list,1)
# Output result
bag_ensemble_prediction<-ifelse(ensemble_predictions>=best_threshold,"Positive","Negative")
cat("Our Bag Ensemble Model Predict This Patient is: ",bag_ensemble_prediction,"\n")
```

### Check if Prediction is Good
(and its accurate!)
```{r}
# Get our first patient's real status & output result
bad_ensemble_real<-ifelse(df_LR_patient$HeartDisease==1,"Positive","Negative")
cat("This Patient's Actual Status is: ",bad_ensemble_real,"\n")
```


# **8. Ensembling 2 (Stack) (Logistic Regression, Random Forest, C5.0)**


## *8.1. Modeling (Final Ensemble 2)*


### Generate a Stack Model Based on all Model Built Before with Test Index
(I use glm generate a model from all model built before)
```{r}
# model_list & df_list are generated in ensemble 1
stack_model<-stack_model_lr(model_list,df_list,test_index,1,"HeartDisease")
```

### Use Stack Model Make Prediction with Test Index
(Make prediction with test index so I can tune model by finding best threshold)
```{r}
result<-stack_model_predict_lr(stack_model,model_list,df_list,test_index,1,0,"HeartDisease")
prediction_value<-result$prediction_value
true_value<-result$true_value
```

```{r echo=FALSE}
# collect information for model evaluation's table construction
# Didn't echo this chunk cause it only serves to collect information
e2_bt_a<-result$accuracy
e2_bt_tpr<-result$tpr
e2_bt_tnr<-result$tnr
e2_bt_F1<-result$F1
```


## *8.2. Tuning (Final Ensemble 2)*


### Find Best Threshold with Test Index
```{r}
best_threshold<-find_best_threshold(prediction_value,df_C5,test_index,"HeartDisease",1,0)
```


## *8.3. Final Validation (Final Ensemble 2)*


### Get Final Performance Result with Validation Index & Best Threshold
```{r}
result<-stack_model_predict_lr(stack_model,model_list,df_list,validation_index,1,0,"HeartDisease",best_threshold)
```

```{r echo=FALSE}
# collect information for model evaluation's table construction
# Didn't echo this chunk cause it only serves to collect information
e2_at_a<-result$accuracy
e2_at_tpr<-result$tpr
e2_at_tnr<-result$tnr
e2_at_F1<-result$F1
```


## *8.4. Use Ensemble 2 Make Prediction*
### First use stack model make prediction (Same patient info I used in bag ensembel)
(Patient info: line 2254)
```{r,results='hide'}
stack_result<-stack_model_predict_lr(stack_model,model_list,df_list_patient,1,1,0,"HeartDisease",best_threshold)
```

### Then output result with best threshold
```{r}
stack_ensemble_prediction<-ifelse(stack_result$prediction_value>=best_threshold,"Positive","Negative")
cat("Our Stack Ensemble Model Predict This Patient is: ",stack_ensemble_prediction,"\n")
```

### Check if Prediction is Good
(and its accurate!)
```{r}
# Get our first patient's real status & output result
stack_ensemble_real<-ifelse(df_LR_patient$HeartDisease==1,"Positive","Negative")
cat("This Patient's Actual Status is: ",stack_ensemble_real,"\n")
```


# **9. Model Evaluation**


```{r echo=FALSE}
# Contruct a table for easy comparison with Model type and accuracy,  
# true positive rate, true negative rate & F1
# I didn't echo this chunk cause it make my evaluation part look messy
model_performance<-data.frame(
  Model=c("Logistic Regression","Logistic Regression","Logistic Regression","Random Forest","Random Forest","Random Forest","C5.0","C5.0","C5.0","Bag Ensemble","Bag Ensemble","Stack Ensemble","Stack Ensemble"),

  Type=c("Cross Validation","Before Tune","After Tune","Cross Validation","Before Tune","After Tune","Cross Validation","Before Tune","After Tune","Before Tune","After Tune","Before Tune","After Tune"),
  
  Accuracy=c(lr_cv_a,bag_lr_bt_a,bag_lr_at_a,rf_cv_a,rf_bt_a,rf_at_a,c5_cv_a,c5_bt_a,c5_at_a,e1_bt_a,e1_at_a,e2_bt_a,e2_at_a),
  
  True_Positive_Rate=c(lr_cv_tpr,bag_lr_bt_tpr,bag_lr_at_tpr,rf_cv_tpr,rf_bt_tpr,rf_at_tpr,c5_cv_tpr,c5_bt_tpr, c5_at_tpr, e1_bt_tpr,e1_at_tpr,e2_bt_tpr,e2_at_tpr),
  
  True_Negative_Rate=c(lr_cv_tnr,bag_lr_bt_tnr,bag_lr_at_tnr,rf_cv_tnr,rf_bt_tnr,rf_at_tnr,c5_cv_tnr,c5_bt_tnr, c5_at_tnr, e1_bt_tnr,e1_at_tnr,e2_bt_tnr,e2_at_tnr),
  
  F1=c(lr_cv_F1,bag_lr_bt_F1,bag_lr_at_F1,rf_cv_F1,rf_bt_F1,rf_at_F1,c5_cv_F1,c5_bt_F1, c5_at_F1, e1_bt_F1,e1_at_F1,e2_bt_F1,e2_at_F1)
 ) 
```

```{r echo=FALSE}
# Use knitr to make a better looking table
# I didn't echo this chunk cause it make my evaluation part look messy
comparison_table<-knitr::kable(model_performance, caption="Table 1. Model Performance Comparison")
```

In this model we construct total 5 models, Bag Logistic Regression Model, Random Forest, C5.0, Ensemble Model Bag that combined all three & Ensemble Stack that generate new model based on all three models. Each of Logistic Regression Model, Random Forest and C5.0 are tested with Cross Validation because its a small data set and we wanna make sure the performance result we get for each model is valid.

```{r echo=FALSE}
# I didn't echo this chunk cause it make my evaluation part look messy
comparison_table
```

By comparing cross validation, un-tuned & tuned result, Logistic Regression, Random Forest & C5.0 shows very small change, meaning the result we get from their performance is valid and trustworthy. 

Among all models, only Bag Logistic Regression show an increase in True Positive Rate (with a little decrease in other feature for sacrifice). I think this happened because my weight calculation and threshold finding are all upon F1 value & F1 favors positive class. But all other shows a decrease, I think this happened by chance because this is a small dataset and it could suggest index selected for testing naturally favors model performance (cause in cross validation, we can always see some model out-performed others). But the bagged logistic regression model bring true positive rate back even with less favored data set. Therefore I think bagged logistic regression model is successfully tuned.

Among all three models, random forest performed the best, before tune & after tune/ with test data set & validation data set. I think this happened because I have a small data set, if this selection favors model, we will have a highly biased result. But random forest is built by bag, which will reduce this kind of bias (especially I choose 500 as tree number). 

Among all three models, C5.0 performs the most poorly. I think its because C5.0 only has 1 tree therefore when data favors C5.0, it performs good but could lead to a result of overfit. Therefore when index is less favorable, it performs bad.

For all three models, we see a balance between tpr, tnr & accuracy. I think this is because our positive & negative samples are close to equal number & I give them 1 for positive weight 1 for negative weight. The model is easier to find a balance for these samples in prediction. 

For bag final ensemble model, its built by calculate the mean of previous model's prediction value. Therefore we can notice each of its performance trait are like balanced value of all previous model. After tune, the ensemble model shows a little increase in true positive rate, this happened because my weight calculation is upon F1 and my threshold tune is upon F1, and F1 naturally favors positive prediction therefore model lean toward positive (which is a good sign).

For stack final ensemble model, its built by generate a new logistic regression model based on previously built model. I think the test index really favors stack model's prediction therefore we could notice it out-performed other model in all metrics. However, might because its a small data set, the stack model is a little overfit toward test index therefore it performs a little worse in validation index. But its metric still lean toward positive class which is a good sign.


# **10. Conclusions**


By comparing these model's performance, I think random forest itself already outperformed others. Stack ensemble model is a little over fit for test index. All models are more favorable toward true positive rate after tuning because I set F1 as the important value to measure model's performance when I tune with threshold & weight. 

I think F1 is a important feature because F1 balance the precision and recall, meaning it won't be too conservative with precise of positive value or be too aggressive to cover all positive value (recall).

Therefore when I use F1 as the most important metric to evaluate a model, the model will lean toward positive value and give higher true positive rate. (Its not very apparent here only because in index splitting, test index is more favorable for model performance so model could be a little overfit toward test make them perform a little worse with validation index. But when considering this, model is actually lean toward positive after tuning). 

For all models above, I think random forest is the one suits the most for this data set and other models all plays a role to drag random forest's metrics down. This happened might because this is a small data set so other models might be bias based on index selection. But random forest is built through bag which can balance this bias. But in reality, I would choose Bag Logistic Regression than Random Forest. Even though random forest gives a balanced and good performance in all metrics, what we are really interested in is positive class. Bagged Logistic Regression gives the highest true positive rate even when data set is less favorable (validation index). Because for clinical purpose, we want to find as much as potential patient (positive value in this case). For that purpose, a sacrifice of negative is acceptable since missing a true heart disease is a much more serious problem than giving patient a false alarm.  


# **Reference**
1. Ellis, C. (2022, August 28). Mtry in random forests. Crunching the Data. https://crunchingthedata.com/mtry-in-random-forests/

